---
toc: true
layout: post
description: Statistical significance post
categories: [markdown]
title: Be aware of statistical significant claims
---

This is a short post on a topic that has recently been malleating slowly in how I think about information, especially "scientific" or data driven results. 

Per wikipedia entry, statistical significance is defined as "a result has statistical significance when it is very unlikely to have occurred given the null hypothesis.....and the p-value of a result, p, is the probability of obtaining a result at least as extreme, given that the null hypothesis is true."

A few housekeeping/definition notes before proceeding: 

A simple example to start with -- "The legal principle of presumption of innocence, in which a supspect or defendant is assumed to be innocent (null hypothesis in not rejected) until proven guilty(null hypothesis is rejected) beyond a reasonable doubt(to a statistically significant degree)"

A null hypothesis -- (in my own words, which might not be as accurate) -- Some statement you think is the base fact of some situation i.e(obserbation). You can also define it as, observing no difference in a population characteristics or the data that is generated.. (still working on a clear definition). For example -- a coin is fair, meaning, over time, heads and tails will be 50%. 

An alternative hypothesis -- suggests or proposes that, you could observe something different in the base fact of some situation. From the coin is fair example, if we were to observe something that would make us reject the null hypothesis -- and say that the coin is not fair.  

Hypothesis testing -- is having some method to reject or say (f..... you) to the null hypothesis within some confidence level i.e -- A lot of science use statistical significance to test confidence level. 

All this means, we gather data so that we can either learn nothing and maintain our "null hypothesis", or we observe something extreme(with some confidence level) to reject the null hypothesis. 


Let's revisit statistical significance.

Saying a result is statistically significant, is asserting that-- Look, I am seeing a pattern in my observations, I have a feeling if there was nothing going on but some random noise, then the chances are less than 1/20 or 5% you will see such a pattern or something as extreme. 

To put it simply, the chances that I am observing something interesting(that deviates from my null hypothesis) as EXTREME as this and its ALL noise(nonsense, or some shit I don't know) are so so small. 

I hope I did justice to this explenation, I am still also still figuring this out, as I believe most people still are and/or confused about the meaning of statisical significance. And people who claim statistical significance can lead you astray if you don't have some framework to reason about it. 

Now, there is a view that the idea of "statistical significance" to interpret some result might be wrong. Some of the arguments are about the techinicalities, that if nothing was going on (just observing some random noise), the chance of observing/finding something that is "statistically significant"(deviates from some null hypothesis) can actually be much greater that 5%.

I will end the post here, because it is an ongoing discussion among a lot of researchers to figure out better ways to test hypothesis and to come up with better metrics. But I hope this stays as a reminder to always be on the look out of "statistically significant" claims, and how much it can be hard to interpret such results based on the idea of just "significance".

Sources: 

https://en.wikipedia.org/wiki/Statistical_significance